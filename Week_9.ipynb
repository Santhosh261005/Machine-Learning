{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOPiQ1pEHic/UqNKFcSQuq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santhosh261005/Machine-Learning/blob/main/Week_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **IMPLEMENT A SIMPLE PERCEPTRON (Coding a Neuron) **"
      ],
      "metadata": {
        "id": "yVZsClOOeSd2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6jUhtlLYFCT",
        "outputId": "24533924-47a7-47d3-8223-92bb3889602a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9990889488055994\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "\n",
        "  def feedforward(self, inputs):\n",
        "    # Weight inputs, add bias, then use the activation function\n",
        "    total = np.dot(self.weights, inputs) + self.bias\n",
        "    return sigmoid(total)\n",
        "\n",
        "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
        "bias = 4                   # b = 4\n",
        "n = Neuron(weights, bias)\n",
        "\n",
        "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
        "print(n.feedforward(x))    # 0.9990889488055994\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basically,we get the output from the perceptron as a boolean value, which is very harsh wrt threshold(i.e., positive if >0 and negative if <0)..which is very harsh...so instead of a yes/no kind of thing...we are using a sigmoid function which gives out a value bw (0,1) ..i.e., the op is much smoother than the step function"
      ],
      "metadata": {
        "id": "N4fER8cCgXtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron with activation Function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "   def __init__(self, learning_rate=0.1, epochs=100):\n",
        "      self.learning_rate = learning_rate\n",
        "      self.epochs = epochs\n",
        "      self.weights = None\n",
        "      self.bias = None\n",
        "\n",
        "   def step_function(self, x):\n",
        "      return np.where(x >= 0, 1, 0)\n",
        "   def fit(self, X, y):\n",
        "      n_samples, n_features = X.shape\n",
        "\n",
        "      # initialize weights and bias to 0\n",
        "      self.weights = np.zeros(n_features)\n",
        "      self.bias = 0\n",
        "\n",
        "      # iterate over epochs and update weights and bias\n",
        "      for _ in range(self.epochs):\n",
        "         for i in range(n_samples):\n",
        "            linear_output = np.dot(self.weights, X[i]) + self.bias\n",
        "            y_pred = self.step_function(linear_output)\n",
        "\n",
        "            # update weights and bias based on error\n",
        "            update = self.learning_rate * (y[i] - y_pred)\n",
        "            self.weights += update * X[i]\n",
        "            self.bias += update\n",
        "\n",
        "   def predict(self, X):\n",
        "      linear_output = np.dot(X, self.weights) + self.bias\n",
        "      y_pred = self.step_function(linear_output)\n",
        "      return y_pred\n",
        "\n",
        "\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "perceptron = Perceptron(learning_rate=0.1, epochs=10)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "test_data = np.array([[1, 1], [0, 1]])\n",
        "predictions = perceptron.predict(test_data)\n",
        "print(predictions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0kXVkQKgQNG",
        "outputId": "22e7b1fa-e4e7-4c54-8f65-94c189ff1a6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([1,1,1,0])\n",
        "perceptron = Perceptron(learning_rate=0.1, epochs=10)\n",
        "perceptron.fit(X, y)\n",
        "\n",
        "test_data = np.array([[1, 1], [0, 1]])\n",
        "predictions = perceptron.predict(test_data)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg3nEnXAmsgZ",
        "outputId": "4251184d-41d4-4220-cc73-346545da722d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation function decides whether a neuron shld fire or not\n",
        "All step functions are activation functions,\n",
        "but not all activation functions are step functions.\n",
        "Here we are using the step function(0/1) as the activation function...we have other activation functions also like sigmoid,ReLU(DL) etc..."
      ],
      "metadata": {
        "id": "D7ddJrCblrj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "\n",
        "# XOR dataset\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "model = MLPClassifier(hidden_layer_sizes=(2,), activation='relu', max_iter=1000)\n",
        "model.fit(X, y)\n",
        "preds = model.predict(X)\n",
        "\n",
        "print(\"Hidden Layer (2 neurons):\")\n",
        "print(\"Predictions:\", preds)\n",
        "print(\"Accuracy:\", model.score(X, y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrM2OPLiop-c",
        "outputId": "de4111cf-3fa3-42dc-f172-417dab4cf0d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden Layer (2 neurons):\n",
            "Predictions: [1 1 1 0]\n",
            "Accuracy: 0.75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XOR is non-linear, and a single-layer perceptron cannot separate the classes.\n",
        "XOR needs a non-linear decision boundary.\n",
        "Single perceptron = only linear boundary.\n",
        "Need Multi-Layer Perceptron (MLP) with hidden layer and non-linear activation (e.g., sigmoid, ReLU).\n",
        "A Multi-Layer Perceptron (MLP) is a feedforward neural network cosisting of ips,hidden layers and ops"
      ],
      "metadata": {
        "id": "CPLOwpCgoVWs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dUuPkIBHpbAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HO7hc18pnnNU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}