{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA8YMJz9JWUChQUHLYYWkU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santhosh261005/Machine-Learning/blob/main/ML_Internal_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Lab Internal-2 Preparation**"
      ],
      "metadata": {
        "id": "Bw8AWa7oeF2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a program that demonstrates the advantage of ensemble learning compared to a\n",
        "single classifier.\n",
        "Apply a Decision Tree and a Random Forest (RF) classifier on a given dataset.\n",
        "Compare their performance using evaluation metrics such as accuracy, precision, recall, and\n",
        "F1-score.\n",
        "Explore the effect of changing the number of estimators (decision trees) in Random Forest."
      ],
      "metadata": {
        "id": "Q5LLyh0zeM4P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMGvzshGeESN",
        "outputId": "67c9f3f1-7c32-4507-9a3b-1d41be96d04a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the decision tree is : 0.94\n",
            "F1 score of the decision tree is : 0.9425287356321839\n",
            "Precison score of the decision tree is : 0.9318181818181818\n",
            "......\n",
            "Accuracy of the RandomforestClassifier is : 0.968\n",
            "F1 score of the RandomforestClassifier is : 0.9694656488549618\n",
            "Precison score of the RandomforestClassifier is : 0.9548872180451128\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_redundant=2, random_state=42)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state = 42)\n",
        "\n",
        "#prediction using single decision tree\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X_train,y_train)\n",
        "tree_pred = tree.predict(X_test)\n",
        "print(\"Accuracy of the decision tree is :\",accuracy_score(y_test, tree_pred))\n",
        "print(\"F1 score of the decision tree is :\",f1_score(y_test, tree_pred))\n",
        "print(\"Precison score of the decision tree is :\",precision_score(y_test, tree_pred))\n",
        "\n",
        "print(\"......\")\n",
        "# Prediction using Random Forest Classifier...multiple decision trees\n",
        "model = RandomForestClassifier(n_estimators = 100,criterion = \"gini\",random_state = 42)\n",
        "model.fit(X_train,y_train)\n",
        "model_pred = model.predict(X_test)\n",
        "print(\"Accuracy of the RandomforestClassifier is :\", accuracy_score(model_pred,y_test))\n",
        "print(\"F1 score of the RandomforestClassifier is :\",f1_score(y_test, model_pred))\n",
        "print(\"Precison score of the RandomforestClassifier is :\",precision_score(y_test, model_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a program that demonstrates the use of simple ensemble techniques: Max Voting,\n",
        "Average Voting, and Weighted Average Voting (assign weights based on each modelâ€™s\n",
        "performance)."
      ],
      "metadata": {
        "id": "L6IKST3YnDNI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ff59dc9",
        "outputId": "d8938a80-9f4d-4985-fcc7-94b3cf06a1ad"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create individual classifiers\n",
        "clf1 = LogisticRegression(random_state=42)\n",
        "clf2 = DecisionTreeClassifier(random_state=42)\n",
        "clf3 = SVC(probability=True, random_state=42) # SVC needs probability=True for average voting\n",
        "\n",
        "# Train individual classifiers\n",
        "clf1.fit(X_train, y_train)\n",
        "clf2.fit(X_train, y_train)\n",
        "clf3.fit(X_train, y_train)\n",
        "\n",
        "# Max Voting\n",
        "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)], voting='hard')\n",
        "eclf1 = eclf1.fit(X_train, y_train)\n",
        "y_pred_max_voting = eclf1.predict(X_test)\n",
        "print(\"Max Voting Accuracy:\", accuracy_score(y_test, y_pred_max_voting))\n",
        "\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Average Voting\n",
        "eclf2 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)], voting='soft')\n",
        "eclf2 = eclf2.fit(X_train, y_train)\n",
        "y_pred_avg_voting = eclf2.predict(X_test)\n",
        "print(\"Average Voting Accuracy:\", accuracy_score(y_test, y_pred_avg_voting))\n",
        "\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Weighted Average Voting (example weights - you can adjust these based on individual model performance)\n",
        "# Let's get the accuracy of individual models on the test set to determine weights\n",
        "acc1 = accuracy_score(y_test, clf1.predict(X_test))\n",
        "acc2 = accuracy_score(y_test, clf2.predict(X_test))\n",
        "acc3 = accuracy_score(y_test, clf3.predict(X_test))\n",
        "\n",
        "weights = [acc1, acc2, acc3] # Using accuracies as weights\n",
        "\n",
        "eclf3 = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)], voting='soft', weights=weights)\n",
        "eclf3 = eclf3.fit(X_train, y_train)\n",
        "y_pred_weighted_avg_voting = eclf3.predict(X_test)\n",
        "print(\"Weighted Average Voting Accuracy:\", accuracy_score(y_test, y_pred_weighted_avg_voting))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Voting Accuracy: 0.9133333333333333\n",
            "--------------------\n",
            "Average Voting Accuracy: 0.9\n",
            "--------------------\n",
            "Weighted Average Voting Accuracy: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Like when a sample is given to predict..it combines the decision of all the three models in case of hard voting and majority one it will give as output...is that true ?\n",
        "Yes, that's exactly right for hard voting (Max Voting)!\n",
        "\n",
        "When you use voting='hard' in the VotingClassifier, for each sample you want to predict:\n",
        "\n",
        "Each individual classifier in the ensemble makes its own prediction (i.e., assigns the sample to a specific class).\n",
        "The VotingClassifier counts the votes for each class from all the individual classifiers.\n",
        "The class that receives the most votes is the final prediction of the ensemble for that sample.\n",
        "It's like a democratic process where each model gets a vote, and the majority wins."
      ],
      "metadata": {
        "id": "UG8T4EDppD4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Write a program to show the difference between Hard Voting and Soft Voting classifiers in\n",
        "ensemble learning using multiple base learners (e.g., Decision Tree, Logistic Regression, and\n",
        "KNN)."
      ],
      "metadata": {
        "id": "0lozDtkApLtQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "632b3729",
        "outputId": "711cc6b5-f247-4a5d-8978-f7d10545e4b5"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Generate synthetic data (if not already available)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create individual classifiers\n",
        "clf1 = LogisticRegression(random_state=42)\n",
        "clf2 = DecisionTreeClassifier(random_state=42)\n",
        "clf3 = KNeighborsClassifier()\n",
        "\n",
        "# Train individual classifiers\n",
        "clf1.fit(X_train, y_train)\n",
        "clf2.fit(X_train, y_train)\n",
        "clf3.fit(X_train, y_train)\n",
        "\n",
        "# Hard Voting\n",
        "eclf_hard = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('knn', clf3)], voting='hard')\n",
        "eclf_hard = eclf_hard.fit(X_train, y_train)\n",
        "y_pred_hard = eclf_hard.predict(X_test)\n",
        "print(\"Hard Voting Accuracy:\", accuracy_score(y_test, y_pred_hard))\n",
        "\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Soft Voting\n",
        "# Note: Not all classifiers support predict_proba. KNN does by default, but you might need to enable it for others.\n",
        "eclf_soft = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('knn', clf3)], voting='soft')\n",
        "eclf_soft = eclf_soft.fit(X_train, y_train)\n",
        "y_pred_soft = eclf_soft.predict(X_test)\n",
        "print(\"Soft Voting Accuracy:\", accuracy_score(y_test, y_pred_soft))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard Voting Accuracy: 0.9\n",
            "--------------------\n",
            "Soft Voting Accuracy: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a program using the RandomForestRegressor model to make predictions on a\n",
        "suitable regression dataset.\n",
        "Enable and observe the oob_score (Out-of-Bag score) parameter.\n",
        "Interpret the results and explain its significance."
      ],
      "metadata": {
        "id": "RMjbqyHqqlNA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58411fea",
        "outputId": "5c8fa4eb-ad74-44e9-8f0d-fc0e399155ed"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X_synthetic and y_synthetic_dummy are available from the previous step\n",
        "# If not, make sure to run the previous cell to generate them.\n",
        "\n",
        "# Split the combined data (original + synthetic) into training and testing sets\n",
        "# Note: In a real application, you would typically apply SMOTE only to the training data.\n",
        "X_train_synth, X_test_synth, y_train_synth_dummy, y_test_synth_dummy = train_test_split(X_synthetic, y_synthetic_dummy, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create individual classifiers\n",
        "clf1 = LogisticRegression(random_state=42)\n",
        "clf2 = DecisionTreeClassifier(random_state=42)\n",
        "clf3 = KNeighborsClassifier()\n",
        "\n",
        "# Train individual classifiers on the synthetic training data\n",
        "clf1.fit(X_train_synth, y_train_synth_dummy)\n",
        "clf2.fit(X_train_synth, y_train_synth_dummy)\n",
        "clf3.fit(X_train_synth, y_train_synth_dummy)\n",
        "\n",
        "# Hard Voting\n",
        "eclf_hard_synth = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('knn', clf3)], voting='hard')\n",
        "eclf_hard_synth = eclf_hard_synth.fit(X_train_synth, y_train_synth_dummy)\n",
        "y_pred_hard_synth = eclf_hard_synth.predict(X_test_synth)\n",
        "print(\"Hard Voting Accuracy (with synthetic data):\", accuracy_score(y_test_synth_dummy, y_pred_hard_synth))\n",
        "\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Soft Voting\n",
        "# Note: Not all classifiers support predict_proba. KNN does by default, but you might need to enable it for others.\n",
        "eclf_soft_synth = VotingClassifier(estimators=[('lr', clf1), ('dt', clf2), ('knn', clf3)], voting='soft')\n",
        "eclf_soft_synth = eclf_soft_synth.fit(X_train_synth, y_train_synth_dummy)\n",
        "y_pred_soft_synth = eclf_soft_synth.predict(X_test_synth)\n",
        "print(\"Soft Voting Accuracy (with synthetic data):\", accuracy_score(y_test_synth_dummy, y_pred_soft_synth))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard Voting Accuracy (with synthetic data): 0.7\n",
            "--------------------\n",
            "Soft Voting Accuracy (with synthetic data): 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rWtn4l20u7tA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}